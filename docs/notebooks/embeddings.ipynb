{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Embedding Visualization\n",
    "\n",
    "This script demonstrates how to:\n",
    "1. Load a trained autoencoder model\n",
    "2. Run inference to compute embeddings (latent representations)\n",
    "3. Visualize the embeddings using scanpy\n",
    "4. Color the visualizations based on expansion status and tissue type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import anndata as ad\n",
    "import matplotlib  # noqa: F401\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "\n",
    "# Set backend to inline for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure scanpy to not show plots automatically\n",
    "sc.settings.verbosity = 3  # verbosity level\n",
    "sc.settings.set_figure_params(dpi=100, figsize=(8, 8), facecolor=\"white\")\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "print(f\"Project root: {project_root}\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from scxpand.autoencoders.ae_models import load_ae_model\n",
    "from scxpand.autoencoders.ae_params import AutoEncoderParams\n",
    "from scxpand.data_util.data_format import load_data_format\n",
    "from scxpand.data_util.data_splitter import get_patient_identifiers\n",
    "from scxpand.data_util.dataloaders import create_eval_dataloader\n",
    "from scxpand.data_util.dataset import CellsDataset\n",
    "from scxpand.util.general_util import get_device, get_new_version_path, load_params\n",
    "\n",
    "\n",
    "# Set up visualization style\n",
    "sc.settings.set_figure_params(dpi=100, figsize=(8, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Get compute device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file for generating embeddings\n",
    "DATA_PATH = project_root / \"data\" / \"scXpand_counts_with_expansion_for_model_08_12_2024.h5ad\"\n",
    "\n",
    "# Path to a trained autoencoder model\n",
    "# Option 1: Use a local model (uncomment the line below)\n",
    "# MODEL_PATH = project_root / \"results\" / \"autoencoder\"\n",
    "\n",
    "# Option 2: Download a pre-trained model from the registry (uncomment the lines below)\n",
    "from scxpand.pretrained.download_manager import download_model\n",
    "\n",
    "MODEL_PATH = download_model(\"pan_cancer_autoencoder\")\n",
    "\n",
    "# To use only a subset of the data for evaluation, set SPLIT_PATH\n",
    "# If SPLIT_PATH is None, the full dataset will be used\n",
    "SPLIT_PATH = project_root / \"results\" / \"optuna_studies\" / \"dev_patient_ids.csv\"\n",
    "\n",
    "# Inference parameters\n",
    "batch_size = 16384  # or 1024 for smaller memory\n",
    "# Folder to save embedding outputs\n",
    "SAVE_PATH = get_new_version_path(project_root / \"results\" / \"embeddings_visualization\")\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Data file: {DATA_PATH}\")\n",
    "print(f\"Results will be saved to: {SAVE_PATH}\")\n",
    "\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in read mode\n",
    "adata = ad.read_h5ad(DATA_PATH, backed=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Data Subset (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_PATH:\n",
    "    # Load a specific subset of patients\n",
    "    with open(SPLIT_PATH) as f:\n",
    "        patient_ids = f.readlines()\n",
    "    patient_ids = [line.strip() for line in patient_ids]\n",
    "    patient_identifiers = get_patient_identifiers(obs_df=adata.obs)\n",
    "    eval_row_inds = np.where(patient_identifiers.isin(patient_ids))[0]\n",
    "else:\n",
    "    eval_row_inds = np.arange(len(adata))\n",
    "\n",
    "assert len(eval_row_inds) > 0, \"No cells found for evaluation\"\n",
    "print(f\"Using {len(eval_row_inds)} cells ({len(eval_row_inds) / len(adata) * 100:.2f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Format and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data format\n",
    "data_format = load_data_format(MODEL_PATH / \"data_format.json\")\n",
    "\n",
    "print(\"Data will be formatted as follows:\")\n",
    "print(f\"{data_format.n_genes = }\")\n",
    "print(f\"{data_format.gene_names[:3] = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model Parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model parameters and initialize\n",
    "params_dict = load_params(MODEL_PATH)\n",
    "prm = AutoEncoderParams(**params_dict)\n",
    "\n",
    "model = load_ae_model(model_path=MODEL_PATH, device=device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded Autoencoder model type: {prm.model_type}\")\n",
    "print(f\"Latent dimension: {prm.latent_dim}\")\n",
    "print(f\"Model architecture: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "eval_dataset = CellsDataset(\n",
    "    data_format=data_format,\n",
    "    row_inds=eval_row_inds,\n",
    "    dataset_params=prm.get_dataset_params(),\n",
    "    is_train=False,\n",
    "    data_path=DATA_PATH,\n",
    "    include_row_normalized_gene_counts=True,\n",
    ")\n",
    "\n",
    "# Create dataloader using helper function\n",
    "eval_loader = create_eval_dataloader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=6,  # Adjust based on your ability to parallelize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(\n",
    "    model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, device: torch.device\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract embeddings from the model's encoder.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_batch, batch in enumerate(dataloader):\n",
    "            if i_batch % 100 == 0:\n",
    "                print(f\"Processing batch {i_batch} of {len(dataloader)}\")\n",
    "\n",
    "            # Get features\n",
    "            x = batch[\"x\"].to(device)\n",
    "\n",
    "            # Forward pass through encoder to get embeddings\n",
    "            latent_vec = model.encode(x)\n",
    "\n",
    "            # If the model returns a tuple (like mean and variance in a VAE), just take the mean\n",
    "            if isinstance(latent_vec, tuple):\n",
    "                latent_vec = latent_vec[0]\n",
    "\n",
    "            embeddings.append(latent_vec.cpu().numpy())\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = extract_embeddings(model, eval_loader, device)\n",
    "\n",
    "print(f\"Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AnnData Object with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new AnnData object with embeddings\n",
    "\n",
    "emb_adata = ad.AnnData(\n",
    "    X=embeddings,\n",
    "    obs=adata[eval_row_inds].obs,\n",
    "    uns={\"embedding_type\": f\"{prm.model_type}\", \"latent_dim\": prm.latent_dim},\n",
    ")\n",
    "\n",
    "# Add metadata for coloring (optional if relevant metadata is available)\n",
    "# emb_adata.obs[\"is_expanded\"] = adata[eval_row_inds].obs[\"expansion\"].map({\"expanded\": \"True\", \"unexpanded\": \"False\"})\n",
    "\n",
    "# Save the embeddings AnnData object (optional)\n",
    "# emb_adata.write_h5ad(SAVE_PATH / \"embeddings.h5ad\")\n",
    "# print(f\"Saved embeddings to {SAVE_PATH / 'embeddings.h5ad'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings with Scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = embeddings.shape[1]  # Get actual dimensionality of embeddings\n",
    "print(f\"Embedding dimension: {latent_dim}\")\n",
    "\n",
    "# Run PCA to reduce dimensionality (helps with visualization)\n",
    "sc.pp.highly_variable_genes(emb_adata)\n",
    "emb_adata.raw = emb_adata\n",
    "emb_adata = emb_adata[:, emb_adata.var.highly_variable]\n",
    "\n",
    "n_pcs = min(emb_adata.shape[1], 41)  # Use a maximum of 40 PCs, or fewer if fewer features are available\n",
    "n_pcs -= 1\n",
    "\n",
    "sc.tl.pca(emb_adata, n_comps=n_pcs)\n",
    "sc.external.pp.bbknn(emb_adata, batch_key=\"sample\", n_pcs=n_pcs)\n",
    "\n",
    "sc.tl.umap(emb_adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"expansion\" in emb_adata.obs:\n",
    "    sc.pl.umap(\n",
    "        emb_adata,\n",
    "        color=[\"expansion\"],\n",
    "        title=\"Expanded T Cells\\nLatent Space Embeddings\",\n",
    "        groups=[\"expanded\"],\n",
    "        na_in_legend=False,\n",
    "        size=2,\n",
    "    )\n",
    "if \"tissue_type\" in emb_adata.obs:\n",
    "    sc.pl.umap(emb_adata, color=[\"tissue_type\"], title=\"Tissue Type\\nLatent Space Embeddings\", size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Save UMAP Coordinates with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with UMAP coordinates and metadata\n",
    "umap_df = pd.DataFrame(emb_adata.obsm[\"X_umap\"], columns=[\"UMAP1\", \"UMAP2\"])\n",
    "if \"tissue_type\" in emb_adata.obs:\n",
    "    umap_df[\"tissue_type\"] = emb_adata.obs[\"tissue_type\"]\n",
    "\n",
    "if \"imputed_labels\" in emb_adata.obs:\n",
    "    umap_df[\"expansion\"] = emb_adata.obs[\"expansion\"]\n",
    "\n",
    "# Save to CSV\n",
    "umap_df.to_csv(SAVE_PATH / \"umap_coordinates_with_metadata.csv\", index=False)\n",
    "print(f\"Saved UMAP coordinates with metadata to {SAVE_PATH / 'umap_coordinates_with_metadata.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
